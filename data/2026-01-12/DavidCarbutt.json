[
    {
        "video_id": "5W1k6kaVpiY",
        "title": "NVIDIA CEO: A 1,000,000X is Coming.",
        "published_at": "2026-01-12T17:09:02Z",
        "sort_date": "2026-01-12",
        "url": "https://www.youtube.com/watch?v=5W1k6kaVpiY",
        "transcript": "Nvidia just dropped 20 billion dollars on a chip company to buy insurance against inference futures they haven't figured out yet. They might also be solving a supply chain problem that even Nvidia can't brute force their way through. I have two clips on this coming up. One from Nvidia CEO Jensen Hang and another from Arc Invest. Then a bonus clip at the end from Jensen where he's predicting a 1 millionx improvement in AI. This is nuts. >> First, I haven't had a chance to talk to you since the Grock licensing announcement. >> A lot of talk about inferencing here. >> Why was that an important deal for you to do given all the advantages you already had in inference? >> Well, we don't compete. Yeah, you're you're absolutely right that that uh evas platform is unique in the sense that we're we're great at training. We're great at post training and now with reinforcement learning and and uh the scale of computing necessary for post- training and because of the investments that we made with NV link and all of the technologies we created, we're incredibly good at inference. We're the only architecture that really spans all three. we've known about Grock for some time and and um the thing that I really like about Grock is although although we never saw ourselves competing um they built a very specialized version of a chip that is that uses integrated SRAM um it can't scale up to the scale that we could scale it's it of course can't be used for training and pre and post training um but if for for low latency high token rate gener generation so long as it fits in the SRAMM, it's really quite quite interesting. And so I really love their low latency focus. Everything about their programming model, their architecture, their system architecture was designed for low latency. >> It's January again, and by now you've started making resolutions to make the new year better for you. But the smartest people are already on the road to master that one skill that matters most in 2026. AI. From just a simple text model in 2019, AI has evolved to detecting diseases, automating work, and even running tasks in your sleep. Looking at these advancements, 2026 is when AI is going to be at its peak and perhaps the last chance for you to get on the AI ship before it sails. And that's why I want to introduce to you the sponsor of this video, Outskill. They are hosting a 2-day AI mastermind training session live this Saturday and Sunday, 10:00 a.m. to 7:00 p.m. Eastern on both days. And right now is the perfect moment to join because you can get in absolutely free as part of the new year upskilling fest. This 16 hours AI training has already built 10 million plus AI first professionals worldwide and is rated 4.9 out of five on Trustpilot. This is where you'll learn how to build AI agents that plan, write, execute, and report for you. Automate workflows that even run while you sleep. Use AI to save hours every week, and get an unfair advantage at work. Not just that, you'll also learn how to profit from these skills. Seats are limited. Use the link in the description to join. Now the extreme this the extreme capability for low latency token generation is also one of the reasons why they had a very hard time addressing the mainstream part of AI factories but in combination with us they don't have to address that you know they no longer have to address the segment of the market that quite frankly where the scale is and maybe maybe with us we can go explore the the edges the fringes of where AI factories could someday. And so I really like the team. I really like Jonathan. Really talented team. I love their conviction. Um I think their architecture has has a uh really unique strengths um for the mainstream part of the market. Some real challenges, but together those aren't their problems anymore. They could really focus on their strengths. Nvidia paid billions for Grock's extreme low latency inference architecture because they're hedging on AI factory infrastructure that doesn't yet exist. Jensen walks through Grock's limitations pretty clearly. The chip uses integrated SRAMM, which means it can't scale to Nvidia's level, can't handle training or post-training workloads and only works well for specific low latency use cases where everything fits in that SRAMM. But he keeps coming back to Grock's laser focus on low latency token generation and how their entire architecture was built around that single goal. Nvidia dominates training. They dominate post training with reinforcement learning at scale. And with Envy Link and their full stack, they're already excellent at inference. They're the only architecture that spans all three phases of the AI life cycle. Grock doesn't compete with any of that. But Jensen still wrote a check in the billions, 20 billion to be precise, because Grock solves one very specific problem incredibly well. And that problem might matter more in 5 years than it does today. Right now, most AI applications can tolerate a second or two delay. You ask Chat GPT something, you wait a moment, and then you get your answer. That's fine for chat bots and code generation, but when you need AI agents making realtime decisions or voice AI that needs to feel like actual conversation or robotics where a half-second delay means the robot crashes into something, suddenly that extreme low latency becomes critical. Grock built their entire chip around solving that problem, even if it meant sacrificing scale and flexibility. Nvidia is betting that as AI moves from cloud-based tools to real time embedded systems, someone's going to need chips that prioritizes speed over everything else. They don't know exactly what those use cases look like. They're buying optionality on a future they can't fully predict. By bringing Grock inhouse, Grock doesn't have to waste resources trying to compete in mainstream AI infrastructure anymore. They can focus purely on their strength, which is pushing the boundaries of how fast you can generate tokens. They didn't need to acquire Grock outright. They just needed to make sure Grock's technology serves Nvidia's roadmap instead of potentially enabling a competitor's alternative architecture. If hyperscalers start building AI factories optimized for different workloads, some for training, some for inference, some for real-time applications, Nvidia wants to own the best solution for each segment. Grock gives them the extreme low latency piece. Even if that piece stays niche for years, Nvidia just locked in dominance across one more dimension of AI infrastructure. Now, Jensen's explanation makes it sound like a straightforward bear, but Brett Winton from ARC has a different read on why Nvidia really pulled the trigger here. Let's start with Gro. Grock with a Q instead of Grock with a K. Um, so Grock is a people call it an ASIC company. It's a specialized AI chip company. um approaching the world in a similar way to Cerebras where they were placing a bet on the idea that um being able to produce tokens quick more quickly um even if you're taking on limitations in terms of number the kinds of models you can run is a way to penetrate the market and so they and and Grock as a investment in our venture strategy um they uh so we believed in in that thesis that kind of speed of inference was going to be net better. Uh Nvidia uh dropped $20 billion on Grock to not acquire it, not even exclusively like license its technology, but to hire a few people including its CEO uh and non-exclusively license its technology. Uh and this is a similar quasi acquisition strategy as was used on Windsurf which was an AI coding company that also got um vacuumed up. Um, and the the idea is you non-exclusively license the technology and you hire the key talent in and so they develop it just for your purposes. So nobody else is going to actually kind of use the IP, but you don't run the risk of regulators saying this acquisition is not allowed. Uh, and um net the um seemingly the the uh shareholders in Grock and the employees are going to get paid out. Uh I think it's most interesting from a what is Nvidia doing here perspective where um the concern about Grock and and these other companies is well yes they have like things that are a slightly different flavor of chips that maybe they can carve out a market niche but Nvidia is occupying the spot where they see all the workloads and they can just uh if if this looks like a useful approach to delivering AI chips they can uh catch up effectively And um Grock um basically was attaching memory on chip. So you can have the entire model loaded really on chip and and therefore do inference very quickly. And um the speculation is actually um particularly given um some of the um the bandwidth or the high high bandwidth memory constraints which is what Nvidia is relying on. Actually that architecture becomes important to running these models. And so Nvidia swooped in and paid $20 billion of which they are generating much more than that in free cash flow uh annually to um acquire in this gro technology and and and um secure their competitive roadmap. So what do you think that you know Cerebrris was going to go public wasn't going to go public now looking >> well according to Pol it was like a 50/50 shot for this year but now it's an 80% chance according to the poly market contract on thin volume but I think people are recognizing one that um maybe just you know grabbing on to like a slightly different um inference architecture uh could be important here. Frankly, I'm I don't know that we know for sure what are what Nvidia's motivations are here. Um they um you know could have been acquiring Grock to make sure that a competitive hyperscaler didn't acquire it and therefore get like an orthogonal vector on them. There's a lot of pointing to again this high bandwidth memory market that has exploded and that is um you know rumored to be uh restricting the ability to produce the frontier model type chips TPUs and Nvidia's um you know uh next generation chips and so it could be that they actually need it to deliver the need this alternative path to navigate around um some supply constraints that they have. Uh, I I think we'll see the degree to which this technology is embedded in in Nvidia's immediate term roadmap versus kind of like it's more medium-term play. Um, and I think it's clear that um the chip space is continuing to heat up. Nvidia paid $20 billion to validate that specialized infant architectures matter strategically and they might need Grock's onchip memory approach to navigate high bandwidth memory shortages that are constraining even the next generation chips. Brett points out that Nvidia is generating way more than $20 billion in annual free cash flow. So this deal barely moves the needle financially. The interesting part is the structure a non-exclusive license. high the CEO and key talent, but technically not acquisition. And Brett speculates the real motivation might be high bandwidth memory constraints that are limiting production of Frontier chips like TPUs and Nvidia's upcoming releases. Gro's architecture sidesteps that bottleneck by loading models directly on chip using SRAMM instead of relying on external high bandwidth memory. So HBM high bandwidth memory has become one of the biggest constraints in AI chip production. Every Frontier model chip needs HPM to feed data fast enough to those compute cores. SK Highix, Samsung, Micron, they're all ramping production, but demand is growing faster than supply. You've got Nvidia, AMD, Google, and every AI chip startup competing for the same limited HBM allocation. And unlike compute cores where you can throw more money at TSMC to get more priority, memory manufacturing takes years to scale. Grotto architecture doesn't have that problem. They put SRAMM directly on the chip, which means the entire model sits right there in fast memory. You don't need external HPM at all. The trade-off is you can only run smaller models, whatever fits on that chip memory. But for inference workloads where you're running the same model millions of times, that architecture starts looking pretty attractive. And if HPM constraints are going to limit how many Blackwell or Reuben chips Nvidia can actually ship over the next 18 months, having an alternative inference architecture that doesn't compete for the same supply chain resources is pretty valuable insurance. We don't know for sure if that's the primary motivation. Nvidia might be buying Grock just to stop a big tech company from owning a different competing way to build AI systems. Or they might be building out multiple product lines for different workloads and wanted Gro's low latency tech in their portfolio. But the fact that Nvidia moved this quickly and paid this much for a company that by Gent's own admission has real challenges for mainstream markets suggests they see something urgent. They might even see possible constraints that make Grock's HBM free architecture more valuable than it seemed six months ago. Brett calls this a quasi acquisition. Same outcome as buying the company, but structured to avoid antitrust scrutiny. You hire the key people, license the technology, non-exclusivity, so nobody else can really use it. And you get all the strategic benefits of an acquisition without having regulators block the deal. They know outright acquisitions of strategic AI infrastructure companies are probably dead in the current regulatory environment. So, Nvidia figured out a new playbook. Pay the shareholders, hire the talent, secure the IP, but keep it technically separate. And boom, you just eliminated a potential competitor and added technology to your road map without filing a single merger document. It's super interesting news and I wanted to give you a breakdown. Next up, Nvidia CEO Jensen Wang's massive prediction on AI getting 1 million times better. This is probably the most important clip of the episode. >> People think that there's this AI um the marginal cost of the AI is going to go go down significantly and it is >> and therefore the AI is going to be dangerous. It's exactly the opposite. If the marginal cost of AI is going to go down significantly, that one AI is going to be monitored by millions of AIs. Mhm. >> And more and more AI is going to be monitoring mon monitoring each other. People don't can't forget that an AI is not going to be an agent by itself. It's likely the AI is going to be surrounded by agents monitoring us. >> And so it's no different than if the if the marginal cost of of keeping society safe was lower, we have police in every corner. >> So one thing that that we were talking about a little bit earlier was just the cost of AI and how it's been coming down. And so >> I I think um in 2024 the the cost of GPT4 equivalent models if you look at a million tokens it came down over 100x um you know somebody in my team did this analysis to show that uh so the costs are dropping pretty dramatically and very rapidly and part of it is all the advancements you all have been driving on the video level but also just across the stack been getting big efficiency gains. >> Yeah. Um, at the same time, model companies were talking about how the costs are rising, how there's enormous sort of capital modes to building these things out. How do you think about cost of training and cost of inference over time and what that means for the average end user or the average startup company trying to compete or people trying to do more in this industry? >> I forget the statistic that but but you know Andre Andre Carpathy um estimated the cost of building the first chatbt I think >> versus now I think you could do that on the PC now. >> Yeah. Yeah. That's probably tens of thousands of dollars at this point or maybe even less, right? And so it costs nothing. >> Mhm. >> And and >> he has an open source project that you can do in a weekend. >> Oh, is that right? Okay. That's incredible, right? >> We're talking about three years. >> Mhm. >> Mhm. >> What people people said cost billions of dollars, um, supercomputers built, raising billions of dollars in order to do all that now >> cost, >> you know, something that you can do on a weekend on a PC. And so that tells you something about how quickly we're making making AI more cost effective >> or Spark. Sorry, probably not quite a PC. Yeah. >> Okay. Not quite a PC. Yeah. We're improving our architecture and performance um every single year. The first ch I think was trained on Voltus. >> Mhm. >> And then uh Ampear um you know and and it wasn't I think the first breakthroughs none of it included Hopper. >> Mhm. And um of course Hopper the last couple two three years and um uh we're off in Blackwell for the last year and a half or so. And um every single one of these generations the architecture improves and of course the number of transistors go up and uh the capacity goes up every single generation very easily every every single year from a computing perspective. The combination of all that getting 5 to 10x every single year >> is not unusual. And here comes Reuben just around the corner. And so we're seeing five to 10x every single year. Well, compounded, it's incredible. Moore's law was two times every year and a half. And over the course of five years is 10x. Over the course of 10 years is 100x. >> In the in the in the case of AI, over the course of 10 years is probably 100,000 to a millionx. >> Okay? And that's just the hardware. >> Then the next layer is the algorithm layer and the model layer. The combination of all that, the fact that if you were to tell me that in the cost in the in the in in the span of, you know, 10 years, we're going to reduce the cost of token generation by a billion times, I would not be surprised. >> Okay. And so that's the tokconomics of of of AI. On the training side, it's not quite as aggressive in in cost reduction, but it's close. If you were to say that that every single year we're increasing by two or 3x over the course of 10 years, incredible. But the important idea is when somebody says it cost $100 million to train something or half a billion dollars to train something. Well, next year it's 10 times less. Next year it's 10 times less. >> People just scale these things up though, right? So the counterargument is, well, we'll just get bigger every year by 10x or 100x or, you know, we'll try and offset that decrease in cost by scale. and others can't keep up. >> Yeah. But really what's happening is is you're and and this is where come in as you know the scale went up by a factor of 10 but the computational burden did not go up by a factor of 10 because you're getting the compounded benefits of all three things. The hardware is going up, the the algorithms of the training models are going up and of course the model architecture is going up and we're getting the benefit of learning from each other. This is, you know, let's face it, Deep Seek was probably the single most important paper that most Silicon Valley researchers read from in the last couple years. >> One of our clients started with zero audience. Now, they're doing $100,000 months thanks to YouTube. And they're not alone. We've helped three businesses hit that level just by growing them a YouTube channel. Want to see how this could work for your business? Book a call with me below."
    }
]