[
    {
        "video_id": "jQbYNvQ2ggg",
        "title": "NVIDIA &amp; Anthropic CEO: This Changes EVERYTHING",
        "published_at": "2026-02-02T12:01:30Z",
        "sort_date": "2026-02-02",
        "url": "https://www.youtube.com/watch?v=jQbYNvQ2ggg",
        "transcript": "Ladies and gentlemen, I've got some spicy clips for you today. In the first clip, we'll see why 2026 might be the year of specialized AI with a lot of examples from companies you've heard before and a bunch of companies you've probably never heard of. Then in clip 2, Anthropic CEO Dario Emodi drops a 40page essay warning that AI might get too powerful too fast and what we need to know about it. And then finally, we'll see the guys from the Moonshot podcast discussing two clips from Dario Modi and Jensen Hang on AI disrupting the economy. I told you it was going to be a juicy show. >> This is your bet for the year. You you think and where you will focus your your investments going forward is in the area of do we call it ASI specialized intelligence as a place to start define that. What is the difference? >> Absolutely. the ad. Caroline, thanks for having me. 2026 is going to be the year all about uh AI capabilities in specialized areas and we got a taste of this in 2025 in some pretty big important categories. Think about self-driving. We now have Whimos on the streets of San Francisco and many other places and many other places and those are operating 10 times safer than the average human driver. 10 times safer. Um, also you see other specialized areas. Anthropic, for example, focused on coding and in 2025 went mainstream with their cloud code product where developers can choose to work with cloud code agents to prototype and even put software into production much faster. We're going to see a lot more of that in 2026. It's going to be specialized capabilities in many new areas. Now, how big are the checks then that you're going to have to write? Because what Whimo, Anthropic, and even like in the legal sector like a Harvey have in common is that they are well capitalized at this point. They had to commit a lot of capital to to train those specialized models in the first place and then deploy them. >> Absolutely. Well, you mentioned Harvey. I'm focused on the early stage investing and in in the case of Harvey, they started really small. um they basically focused on an area and they said, \"Hey, this is an area we're going to be deep experts in.\" That's the legal area. And they focus on the the actual user obsession. They now have over a thousand customers uh for the legal tech space, and they are kind of a a right-hand person to the lawyer. In fact, one of their customers said, \"I would sooner lose my coffee for the year than my Harvey license.\" My sister's a lawyer. She uses Harvey. She'd probably say the same exact thing. 2026 we're going to look back at the end of the year and realize how many areas in AI we just couldn't live without. >> Constantine where is next therefore because you actually already have made these bets in these very focused areas and applications of AI and we just named Harvey but you're also in 11 Labs if you're thinking about the way in which we're going to interact with it from a vocal perspective and and create ourselves but where's next where hasn't been disrupted? >> Absolutely. I'll give two tastes of what's coming next. one in the physical world and then one in the digital world. Uh so in in the physical world we're investors in a company called Vercata. They are the leading AI physical security uh company and they they're basically an extension to those physical security guards who walk around a premise and can now monitor an even larger premise for physical security. Uh they help investigations go 30% faster, dramatically faster. Uh, and that sounds like a statistic, but let's make it very real. They were working with a major airport that had a bomb threat, and they were able to help close that investigation significantly faster, which means that the uh airplanes actually boarded, flights got out on time, and passengers got to their destination. We're going to see a lot of that now in the the digital world. We work with a company called Expo. It's a AI penetration tester. They set out to get really good at finding vulnerabilities in websites and applications. Uh within six months, they became as good as a human penetration tester, a professional at finding vulnerabilities. Within 18 months, they've actually become best in the world. They're top ranked on Hacker One, which is the global leaderboard for individual hackers. And they're finding major vulnerabilities at companies, reporting those to the companies, helping them patch them so that the bad guys don't get there. will have a lot more capabilities like that in 2026. >> If you want to understand how these specialized AI applications actually work under the hood, the models, the training, the approaches, the competitive dynamics, I put together a course called the AI landscape that breaks down all of this, the links in the description. You'll get lifetime access, and the price will be going up as we add new modules, which we're thinking of doing soon. Back to the show. Specialized AI companies are building competence in narrow domains faster than generalist models can catch up and that creates a compounding advantage. So Constantine's framing here is pretty interesting because he's making a distinction that a lot of people aren't dialing into. the big foundation model companies, OpenAI, Anthropic, Google, they're all racing to build the smartest general purpose AI. But he's betting that the real money in 2026 comes from companies that pick one problem and go incredibly deep on it. And when you look at his examples, the pattern becomes clear. Whimo took a decade and billions of dollars to get 10 times safer than humans. Harvey spent years obsessing over how lawyers actually work. Not just legal knowledge, but the workflows, the document review, the way law firms bill clients. That's why someone at a law firm says they'd rather lose their coffee for a year than their Harvey license. The security company he mentions, Expo, is probably the most telling example. They set out to find software vulnerabilities, and with 18 months, they became the best in the world at it. Top ranked on the global leaderboard ahead of human hackers who've been doing this for decades. This is because they pointed everything at one problem and kept iterating. And this creates a flywheel that's hard to break. When you're the best at one thing, you get more customers in that domain, which means more data from that domain, which means you get better faster than anyone trying to be good at everything. So Constine's not just betting on specialized AI. He's betting that focus beats breadth when you're trying to solve hard problems. So that's the opportunity side of AI in 2026. Now, let's talk about what happens when AI gets too good. Dario Modi runs Anthropic, the company behind Claude, and he just published a 40page document about AI safety that's been circulating everywhere. And what makes this different from the usual AI doom stuff is that Dario is not some outsider warning about technology he doesn't understand. He's a legend in the AI space. So, when he's saying he's worried, it's probably worth paying attention to. You know, I've been in the AI industry for a long time. I was at Google. I was at, you know, I led research at Open AI for several years. So, I've seen most of the companies that are leading in the AI space now. And I've been following AI almost almost since the beginning of when, you know, gen what we call generative AI, which is, you know, today's today's kind of generation of AI models started. And the thing I most noticed is that the cognitive abilities of these AI systems would grow year after year. In the 90s, we had something called Moors law, which meant chips got faster and faster. It feels like now we have almost a Moore's law for intelligence. And so, I'm watching the cognitive abilities, the intelligence of these models tick up day after day, year after year. And and you know, in that time from 2023 to 2026, we've gone from maybe the models being like a you know, a smart high school student who's good at some things and not others to really starting to get at the PhD level. When we look at what the models are able to do in terms of coding, which you mentioned anthropic, is focused on, but also things like biology and life sciences, the the potential of what the models can do is is is incredible. You know, we're starting to work with pharmaceutical companies. I think these models could cure cancer, but but on the other hand, you know, things that are that are very smart, this is a lot of power in our hands. And so, a lot >> So, Dar, I want to ask you, you're a busy guy. Why why write this, right? It's 40 pages. It's it's pretty dense. At times it's it's scary. At times it's hopeful. At times it's empowering. I mean it's it's a fascinating read. Did you use Claude to write this at all? >> I uh you know I I use Claude to give me ideas and to help with the research, but the actual writing the actual writing is mine. I don't think is quite good enough yet to to write the whole the whole thing. But I you know I I definitely used it to to improve my ideas. >> So Claude's not as smart as the creator. But I I I don't know if that's what you were saying, but you were saying something similar to that. I I'm curious >> help in some ways. >> Yeah. I I'm I'm curious what happened to you. What did you see recently that inspired you to want to write this essay? And who is this essay for? >> Yeah. So, uh you know, in terms of in terms of what inspired me, I think the fact that the models are writing code, including at Enthropic, I have engineers within Enthropic who say, I don't write any code. I don't write any code anymore. um I just let Claude write the code and you know I edit it or I look it over and of course at Entropic writing code means designing the next version of Claude itself. So we essentially have Claude designing the next version of Claude itself. Not completely, not in all ways but in many ways that loop starts to close very fast and and and so I look at this and I say wow this is exciting. It's incredible what we can do with the world but also it's really speeding up a lot and I'm you know I'm I'm just not sure we have that much time. Yeah, Dario, I want to dig a little deeper into the essay now for our viewers. You lay out the five things you worried about when it comes to powerful AI, right? And just for our viewers, powerful AI is the next level of AI, which you think we'll get there possibly in a few years. I want to run through them briefly here. Number one, autonomy risks, right? Could the AI dominate the world with superior weapons? Number two, misuse for destruction, where you talk about terrorists who create biological weapons using AI. Number three, the misuse of for seizing power. Governments using AI for mass surveillance and propaganda. Some will say that that could be happening now. Number four, economic disruption and ensuing unemployment, which again happening right now. And number five, the indirect effects of rapid development and the destabilization that could cause. I mean, some of these, like I said, they're happening now. Some of these other ones you lay out that that was the plotline for Terminator. I mean, is is that are these the realities? >> Yeah. So, you know, what I've said with with all of these, and I say it in the essay, is, you know, our our view into the future is very cloudy. We're not sure what will, you know, we're not sure how many of the benefits will materialize. We're not sure how many of the risks will materialize, but because we're going at it so fast, I think there's value in writing up a document that doesn't say we're doomed. All these five terrible things are going to happen, but these are some possibilities. You could think of it like a threat report, you know, like like you know, United States has a bunch of plans for, you know, what if what if this happens? What if a terrorist gets a nuclear weapon? Doesn't mean all those things are going to happen, but it means they could happen. And so we need to be prepared for them. And and yeah, the idea that AI models might have motivations that, you know, that we don't trust, that aren't that aren't aligned with humanity. We do everything we can, we make them, to make that not true. Winthropic has done a bunch of stuff, you know, to make the models more reliable, more steerable, more stable, but but there's something about the way we make AI models. It's less like programming a computer. It's more like growing a plant or an animal. And so there is some amount of unpredictability. And I'm warning about this possibility not because I think it's inevitable, not because I think we're doomed, but but because I want people to understand the ser to take seriously how much they have to control this technology, how much they have to, you know, take seriously the problem of making it work, testing it out, seeing what might go wrong, in some cases, getting the law involved to make sure that AI companies like us behave responsibly and don't impose these burdens on society. Anthropic engineers are already using Claude to design the next version of Claude, which means AI improvement is starting to compound on itself. So, the thing that jumped out to me in this interview wasn't the five risks stereo listed, the autonomous weapons, the bioweapons, the surveillance stuff. That's the standard AI safety playbook. What actually matters is the engineering detail he dropped in the middle. He said engineers at Anthropic don't write code anymore. They let Claude write it and then review it. And the code Claude is writing includes the code for the next version of Claude. That's a feedback loop that changes everything about how fast AI improves. Previously, AI got better because humans had new ideas, ran experiments, trained new models. The bottleneck was human ingenuity and human time. Now, the bottleneck is starting to shift. If Claude can contribute meaningfully to its own deployment, then improvement compounds in ways that are hard to predict. Not because Claude is secretly plotting anything, but because the system can iterate faster than any purely human team. And Dario's honest about the uncertainty here. He's not saying we're doomed. He's saying we're moving fast enough that we should take the possibility of problems seriously. His comparison to growing a plant instead of programming computer is actually pretty useful. When you program software, you control exactly what it does. When you train a neural network, you're more like a farmer. You set the conditions and hope the crop turns out right. That unpredictability matters a lot more when the crop is getting smarter every generation. The other thing worth noting is what Darrow thinks will change behavior. He predicts that AI companies will get much more paranoid about safety once AI starts feeling powerful. Not when researchers say it's powerful, but when the engineers building it every day start getting nervous about what they're creating. We're not necessarily there yet, but according to the guy running one of the leading AI companies, we might get there faster than most people expect. Now, let's see Jensen Wang and Dario again on this clip from Moonshot podcast. >> Let's begin with Dario Amade, the CEO of Enthropic. He was a rock star during Davos and Jensen Wang. Let's listen to Dario first on on the economy. If you if you look at what AI is capable of, if you have these models that are getting more and more capable across a wide range of cognitive tasks, you know, you you look at all labor in the economy, that's something like $50 trillion a year. So I could easily imagine that the revenue of the industry or even single companies, if it's even 10% of that, could be $5 trillion a year. Now, that's something we haven't seen in the history of the world. That creates all kinds of problems as well as creating all kinds of growth. And on to Jensen Wong, CEO of Nvidia here. >> The largest infrastructure buildout in human history. We're now a few hundred billion dollars into it. That's it. >> We're a few hundred billion dollars into it. Uh Larry and I, we get the opportunity to work on many projects together. There are trillions of dollars of infrastructure that needs to be built out. And it's sensible. It's sensible because all of these contexts have to be processed so that the AI so that the models can generate the intelligence necessary to power the applications that ultimately sit on top. And so we have chip factories, computer factories, and AI factories all being built around the world. So I saw a text this morning uh from Elon saying he expects to see hundred trillion dollar company valuations coming up in the next you know few years. I think he said 2030. Uh when asked which company it was uh spe like the combination of SpaceX and Tesla. Um so you know a trillion here a trillion there. See what are you thinking about these numbers being thrown around in the economy? I start to look at this as becoming meaningless right in an abundance environment uh kind of whether it's 10 trillion or 100 trillion um it's the whole thing becomes arbitrary uh and is not a real gauge of strength. Um, what I found really interesting about the Nvidia and the um, Daario uh, conversation was they're on opposite ends of the stack and they're kind of really saying roughly the same thing because comput's becoming the new oil and the new electricity and this is really just incredible stuff. It was amazing watching the output of the overall event from a distance. >> Well, just in terms of tone too, Jensen sounds like he's talking to kindergarten. you know, he's talking so slowly and and you know, things that we talked about on the podcast, you are a hundred times more detailed and sophisticated. That that gives you a sense of the audience like he's the the world >> he's talking to government people. He has to speak to kindergarten level. >> And Dario's example of the look, the the global labor market is 50 trillion. Everybody get that? Okay. 5 trillion is an extreme lower bound of the part that can move to AI today. That's that's incredibly low. But even that justifies everything we're talking about now. You guys get it? You know, it's it's that cadence of like, please try to keep up with me here. So, there's a lot of that in this particular forum. >> In my mind, this is all borderline obvious. Of course, capital in the form of AI is going to continue to substitute for labor. And of course, trillions of dollars of AI infrastructure capital buildout will be needed to substitute for ultimately tens maybe eventually hundreds of trillions of dollars of services and human labor per year. So I I think to Dave's point, this really is just spelling out the basic arithmetic of what a posthuman economy looks like. >> Yeah. A trillion here, a trillion there. >> Ads are expensive and people don't trust them anymore, but they do trust YouTube. That's why three of our clients now make $100,000 a month for their business from growing a YouTube channel. If you run a business, book a call with me and I'll help you map this",
        "summary_hu": "A videóban az AI-specializált cégek 2026-ra várható növekedéséről és potenciáljáról beszélnek, amely során a különböző szakmai területeken, mint például jogi vagy biztonsági technológia, új alkalmazások jelennek meg. Emellett Dario Emodi, az Anthropic vezérigazgatója figyelmeztet arra, hogy az AI gyors fejlődése komoly kockázatokkal is járhat, beleértve az autonóm fegyverek és a gazdasági zűrzavarok felmerülését. Az előrejelzések alátámasztják, hogy a vállalatok, amelyek egy adott problémára összpontosítanak, versenyelőnyhöz juthatnak a szélesebb értelemben vett AI modellekhez képest.",
        "summary_en": "The video discusses the anticipated growth and potential of specialized AI companies by 2026, with new applications emerging in various professional areas like legal tech and security technology. Additionally, Dario Emodi, CEO of Anthropic, warns that the rapid development of AI comes with significant risks, including the emergence of autonomous weapons and economic disruption. The forecasts support the idea that companies focusing deeply on specific problems may gain a competitive advantage over broader AI models.",
        "crypto_sentiment": "Neutral",
        "sentiment_score": 50,
        "key_points_hu": [
            "2026-ban az AI-specializált cégek dominálnak majd a piacon.",
            "Dario Emodi figyelmeztet az AI fejlődésével járó kockázatokra.",
            "A mélyreható specializáció előnyhöz juttathat egyes vállalatokat a piacon."
        ],
        "key_points_en": [
            "Specialized AI companies are expected to dominate the market by 2026.",
            "Dario Emodi warns about the risks associated with AI development.",
            "Deep specialization may give certain companies a competitive edge in the market."
        ],
        "main_topics": [
            "Specialized AI Development",
            "AI Safety Concerns"
        ]
    }
]