[
    {
        "video_id": "7fk2hUJwVoo",
        "title": "Jensen Huang Just Revealed The Next Phase",
        "published_at": "2026-01-08T12:00:30Z",
        "sort_date": "2026-01-08",
        "url": "https://www.youtube.com/watch?v=7fk2hUJwVoo",
        "transcript": "Jensen Huang, Nvidia CEO, just took to the stage at CES and has revealed something I don't think most people are noticing at the moment. I'm breaking down why I think this moment marks the official transition from AI 1.0 to AI 2.0 and the specific breakthrough that told Jensen humanoid robotics would finally work. Less than 1 hour, Nvidia CEO Jensen Wong takes the stage at CES for his closely watched keynote address. Wed Bush's Dan Ies joins us now with what we might hear. It's good to see you. You are there as one would expect you to be. What's he going to say today? >> Look, I mean there's only one godfather of AI, right? And it's Jensen. I think he's going to lay out specifically around robotics autonomous. Look, this is the next chapter of AI that I think Jensen's going to talk about and obviously I think he's also going to hit on just the demand that he's seeing and no one has a better vantage point and Scott I think really what it does this kicks off the next sort of layer of the AI revolution when it comes to robotics autonomous I think that's something that's not factored into Nvidia stock and I don't think it's factored in to a lot of these other tech names Qualcomm among others so so he's ready to move on you think to AI I 2.0, if you will, where where we move past all of the talk specifically about that company's chips and about how they can better be utilized in the areas that you're talking about. >> Yeah, I think this is really him planting the flag, showing that Nvidia, it's not just about chips, the 12:1 demand that we've seen obviously fueling the AI revolution. It's showing that they're going to be endto end when it comes to robotics. They're going to be such a huge player when it comes to autonomous and like we've talked about for Tesla, this is the year of autonomous. It's the year of robotics and I think it all starts with Jensen because no one has a better vantage point. I think what he weighs out here is really going to dictate strategically what we see on the consumer AI revolution that hasn't even started yet. We're witnessing the official transition from AI 1.0 to AI 2.0. The first phase was about training language models and building data center infrastructure. The next phase is robotics and autonomous systems operating in the physical world. Most investors haven't adjusted their mental models for what's about to come. Dan Ives calls this the next chapter of AI and the next layer of the AI revolution. He highlights robotics and autonomous systems as the focus and he makes a critical point. This shift isn't factored into Nvidia's stock price or broader tech themes. The market is still pricing these companies based on the old paradigm. AI 1.0 outputs were digital. Text on screens, generated images, code suggestions, everything happened inside computers. The AI could think but couldn't touch anything. AI 2.0 is fundamentally different. The outputs are physical. Robots moving through warehouses, cars driving themselves, humanoid machines picking up objects. The AI generates actions in the real world. When AI controls robots and vehicles, the addressable market is measured in everything humans currently do with their bodies. Manufacturing, construction, healthcare, logistics, agriculture, the scope expands by orders of magnitude. Analysts trained on chip cycles, and enterprise software spending can't model this properly. Dan Ives uses a phrase that deserves attention. He says Nvidia is becoming endto-end in robotics. That means they're providing the full platform, hardware, software, simulation environments, and development tools. Component suppliers compete on specs and price. Platform providers capture value across the entire ecosystem and become impossible to replace. This is why Jensen's keynote matters so much. He's declaring Nvidia's intent to own the physical AI platform before most competitors recognize physical AI is even a distinct category. Platform positions get established early. Once developers commit, switching costs become enormous. The market hasn't priced in that Nvidia is making the same strategic move Apple made decades ago. Control the whole stack. Capture all the value. If you're curious about this transition from AI 1.0 to 2.0 know and want to know more about every major process in the world of AI. I built a course that teaches everything from first principles to more advanced ideas like physical AI. And I did it in a simple, approachable way. You get lifetime access and pricing goes up each time we add new modules. I'm thinking about adding a new module soon. So, if you want in at the lowest price, now is that moment. Check the link in the description below for more. Now, let's hear Genton explain exactly why he knows this moment is different from all the previous robotics hype cycles. >> Robotics, it's been a dream part of CES for a long time, but it's also been elusive and we had IR robot kind of not go in a great direction. Um, granted, we do have autonomous vehicles now and those are really robots, I guess, right? As we're talking about physical AI, >> what makes this moment different? X4 robotics in particular and maybe give me humanoid because we tend to get very excited about it but I wonder too excited. >> Excellent. Excellent. Timing is everything. We've been thinking about this area for a long time. waiting for that moment. And and um as in many of the things that we do um whether it's digital biology and turning drug discovery from a from a discovery process to an engineering process, from a scientific process to a science and engineering process to self-driving cars to even the work that we do in computer graphics, rate tracing is now completely done in real time. It took us 30 years to do it in real time. And so you have to pursue something for a long period of time looking for that moment where that enabling technology is discovered. In the case of human robotics, let me let me first say that a computer doesn't know and doesn't care what kind of tokens is generating. It could be generating a language token. They could be generating a video token. They could be generating a steering wheel activation token. or it could be generating a finger articulation grasp token. The computer in the final analysis is just a bunch of numbers. >> The moment that I saw generative video, us texting into a prompt, two people sitting having a conversation. The guy wearing a jacket reaches out, picks up a cup of water, and drinks it. I could describe that into a generative AI model today and you and I mo both know that Chris for example Yeah. >> Right. With Runway at Runway. Yeah. >> I could give him that prompt and he will generate an amazing video. Isn't that right? And that video has a person reaching out and picking up a cup. Why is that model different >> than a generative model for a human or robot picking up a cup? The moment I saw that working that well, then the rest of it is still a whole bunch of research, a whole bunch of whole bunch of technology, but you could tell that the enabling technology is just right around the corner. >> Jensen revealed the exact moment he knew humano robotics would finally work. When he saw generative video models correctly generate a person reaching out and picking up a cup, he realized the enabling technology had arrived. That seemingly simple video generation proves AI has already solved the hardest problems in robotics. Jensen describes the breakthrough clearly. Prompt a generative AI with a simple description. Two people having conversation. A guy in a jacket reaches out, picks up a cup of water and drinks it. Runway can generate this today. Then he asked the key question. Why is that model different than a model for a robot picking up a cup? The moment he saw video generation working that well, he knew the enabling technology was just around the corner. Jensen makes a critical point about how AI actually works. A computer doesn't know or care what kind of tokens it generates. Language tokens, video tokens, steering wheel tokens, finger articulation tokens. The computer just sees numbers and predicts more numbers. The architecture doesn't know it's working with words or pixels or motor commands. This means every improvement in language models indirectly improves robotics. The breakthroughs are transferable across every domain. When AI generates realistic videos of a hand grabbing a cup, it has already solved most of the hard problems in robotics. It understands how hands work, which fingers wrap where, how grip tightens gradually, how wrists rotate. It understands physics, weight, momentum, gravity, and friction. If any element were wrong, the video would look fake. Our eyes catch physics errors instantly. Photorealistic video generation proves the model learned a rich representation of physical reality. Jensen's insight is that the same understanding generating video of a hand grabbing a cup could instead generate motor commands for a robot hand grabbing a real cup. The scientific question was whether neural networks could learn physics from observation. Video answered that definitively. The remaining work is translating pixels to motor commands. That's engineering, not research. Vision systems, precise execution, feedback loops, and safety systems. Robotics companies now know how to solve those problems. What they didn't know was whether AI could learn physical intuition. Video generation proved it can. The starting gun has fired. Most people pour money into ads people ignore. YouTube changes that. It builds trust, authority, and a real connection at scale. One law firm we worked with landed 33 clients in just 5 months worth $330,000 from their YouTube channel. If you run a business, this is one of the most overlooked opportunities right now. Book a call with me below and I can show you how we can make it"
    }
]