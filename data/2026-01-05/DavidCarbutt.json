[
    {
        "video_id": "0rws5efYalk",
        "title": "Chamath Just Made BILLIONS!!!",
        "published_at": "2026-01-05T16:22:56.000Z",
        "sort_date": "2026-01-05",
        "url": "https://www.youtube.com/watch?v=0rws5efYalk",
        "views": 1490,
        "duration": "00:22:07",
        "transcript": "Chamath just had one of the biggest wins of his career. Nvidia just signed a licensing deal with Grock, a company he backed for nearly a decade through multiple near-death moments for around $20 billion. This wasn't luck. It was a brutal non-conensus bet on a high school dropout. And that's exactly why it worked. We have one thing we need to get to here on the docket, which is a victory lap for our besty Chimath Polyhapita. This week, Nvidia entered into a licensing agreement to uh license Brock's technology for inference chips. Chimath was the backer of this company for close to a decade and um had to come rescue it a couple of times. It was a brutal non-conensus bet. uh you invested in Grock essentially formed the company with your partners 5 years 6 years before chat GBT even debuted. So this was a visionary thing to do and it was a hard bet. The Grock deal proves that backing exceptional founders through years of uncertainty can be chasing obvious winners that everyone else already sees. Chimath has called this 8 years of plotting through the wild wilderness with a lot of misfires. And to put things in context, Chimath invested in Grock five or six years before chat GPT even existed. This was a time where AI chips wasn't even a category people were funding. Backing the right founder matters. And what matters more is whether they've already proven they could build something no one else could. Jonathan did that at Google before Grock even existed. >> I want to take this opportunity to introduce Jonathan. Obviously, a lot of you guys have heard of his company, but may you may not know his origin story, which is quite honestly, having been in Silicon Valley for uh 25 years, one of the most unique origin stories and founder stories you're going to hear. Um, we're going to talk about some of the things that he's accomplished both at Google and Grock. We're going to compare Grock and Nvidia because I think it's probably one of the most important technical considerations that people should know. This is a not an overnight success story. This is a eight years of plotting through the wilderness uh punctuated frankly with like a lot of misfires. Um which is really the sign of a great entrepreneur. Um so but I want people to hear the story. Jonathan may be there. You guys have all heard about entrepreneurs who have dropped out of college to start billion-dollar companies. Jonathan may be the only high school dropout to have also started a billion-dollar company. Before Jonathan Ross started Grock, he built the TPU at Google, a side project funded from leftover budget money. He was competing against a team led by a touring award winner. He won and that chip became the foundation for everything Google does in AI today. I started what was called the TPU as a side project and it was funded out of what a VP referred to as his slush fund or leftover money >> and it was never expected. There were actually two other projects to build AI accelerators. It was never expected to be successful which gave us the cover that we needed to do some really counterintuitive and innovative things. Once that became successful they brought in the adult supervision. >> Okay, take a step back though. what like what problem were you trying to solve in AI when those words weren't even being used and what was Google trying to do at the time where you saw an opportunity to build something? >> So this started in 2012 and at the time there had never been a machine learning model that outperformed a human being on any task and the speech team trained a model that transcribed speech better than human beings. The problem was they couldn't afford to put it into production. And so this led to a very famous engineer, Jeff Dean, giving a presentation to the leadership team. It was just two slides. The first slide was good news. Uh machine learning works. The second slide, bad news, we can't afford it. So they were going to have to uh double or triple the entire uh global data center footprint of Google at an average of a billion dollars per data center, 20 to 40 data centers. So 20 to40 billion. And that was just for speech recognition. If they wanted to do anything else like search ads, it was going to cost more. That was uneconomical. And that's been the history with inference. You train it and then you can't afford to put it into production. >> So against that backdrop, what did you do that was so unique that allowed TPU to be the one of the three projects that actually won? The biggest thing was Jeff Dean noticed that the main algorithm that was consuming most of the CPU cycles at Google was matrix multiply and we decided okay let's accelerate that but let's build something around that and so we built a massive matrix multiplication engine when doing this there were those two other competing teams they took more traditional approaches to do the same thing one of them was led by a touring award winner and then what we did was we came up with what's called a systolic array. And I remember when that turning award winner was talking about the TPU, he said, \"Whoever came up with this must have been really old because systolic arrays have fallen out of favor.\" And it was actually me. I just didn't know what a systolic array was. Someone had to explain to me what the terminology was. It was just kind of the obvious way to do it. And so the lesson is if you come at things knowing how to do them, you might know how to do them the wrong way. It's helpful to have people who don't know what should and should not be done. >> Jonathan Ross built the most important AI chip in Google's history as a side project, and he did it by not knowing what was supposed to be possible. Jonathan explained that in 2012, Google's speech team had trained a model that transcribes speech better than humans. Jeff Dean, one of Google's leading engineers, presented two slides to Google's leadership. The good news, machine learning works. The bad news, we can't afford it. Deploying it would have required 20 to40 billion in new data centers. Think about that timeline for a second. In 2012, no one was talking about AI. No chat GPT, no AI hype cycle, no trillion dollar valuations for chip companies. Google had just proven that machine learning could beat humans at a real task and they couldn't even afford to ship the product. That was the moment Jonathan Ross stepped in. He started the TPU as a side project. His VP called the funding source a slush fund. Nobody expected it to work and two other teams at Google were building AI accelerators at the same time. Jonathan was a high school dropout working on something that wasn't supposed to succeed. That side project now powers a deca billion dollar a year cloud business at Google. Google Cloud runs mostly on TPU accelerators. They've got pods of 10,000 chips training frontier scale models for search, video, and language. The architecture Jonathan created outperforms GPUs by 50 to 100% per dollar on specific AI workloads, which is why Meta is now in talks to spend billions of dollars buying TPUs for their own data centers. Anthropic chose Google as their primary infrastructure partner and reserved up to a million TPUs. Open AAI, Google's direct competitor, also just announced that they'll use Google Cloud to help power Chat GPT. When your rivals are buying your hardware, you've built something that can't be ignored. Google also recently just unveiled their 7th generation TPU. That's 13 years of iteration, building on the foundation Jonathan made in 2012. Google's now spending 85 billion on infrastructure this year alone, the largest single-year investment in tech history. And about twothirds of that goes towards chips, TPUs, and data centers. When Schmath backed Grock, he was betting on the guy who already built the architecture that Google, Meta, Ananthropic, and OpenAI all depend on today, but he didn't know that at the time. Now, let's turn to Chimath and his thoughts on what he's managed to accomplish. >> We started the business 10 years ago, and the whole idea was just observing that there was these patterns that was happening inside of AI that we wanted to be ready for. And I think that this is going to be really important over the next 5 to 10 years. So to the extent that you're either a technologist or an investor, let's just take 30 seconds. So in a world of kind of these LLMs, there's two terms that I think you're going to hear a ton about over these next few years. The first term is prefill and the next is decode. Gavin Baker had an excellent tweet that broke this down. Nick, you can include that in the show notes. What prefill and decode are is two very distinct ways of how models think and how a model goes through the process of answering a question that you ask it. And so when you send a prompt to AI, what happens is that the model processes it. This is called the reading phase or prefill. It reads your entire prompt all at once and then it does a bunch of math. Okay? Calculates all these relationships between all the words and it stores them in temporary memory. The problem is that this is really compute bound. So it requires massive brute force and NVIDIA GPUs crush you and their architecture is designed for massive parallel processing which makes them really amazing at digesting these long prompts, right? And Sax and a bunch of other people have talked about context windows growing and growing. So the problem just gets bigger and bigger. Nvidia just completely dominates. But the next phase though, this critical phase, the decode phase is the writing phase, right? So the model starts to generate a response. You ask it a question and its response one token at a time and then to pick the next token to pick the next word it has to look back at everything it has said already so that it doesn't hallucinate. The problem is that this is incredibly memory bandwidth constrained. The math of all of this is easy but then you got to do all of this nonsense. Imagine like a building and from getting from point A to point B, you got to take an elevator, go up to the 10th floor, take the next elevator, go back down to the ground floor, then walk across. That would never make any sense. You'd want to just walk across the hallway, right? And in our architecture a long time ago, we made these design decisions from day one. We said, you know what, there's an entire technology stack that Nvidia and Google have pioneered. If we try to go and build a building that looks like that, we will fail. they're too smart, they're too capitalized, they're too good, they're too big, they'll get the best deals and we'll just be shut out. And so what we did was we took a very different architectural approach. We took a very conservative process technology. We weren't pushing the boundaries of physics and we used a lot of what's called SRAMM, so memory on the chip so that we could do this decode thing as well or better than everybody else. And so now when you put these two things together, I just think it's going to create a huge acceleration in the ability for this entire infrastructure layer to get much cheaper and much more valuable, which I suspect then it'll have a lot more developer pool. You'll get a lot more applications being built, billions and billions of more people using it. So think about it as pre-filled decode. We were really great at decode and I think partnering up was the strategic rational. And so essentially what happened was last May Nvidia announced this thing which allowed their chip to talk to other things and our team reached out and said hey can we experiment to see if our chip could talk to your chip and we could do this thing better. And over the summer there was some kind of like spreadsheet work that happened. Then they were interested enough where they were like, \"I'll give you a couple engineers.\" And Jensen was kind of curious about it. And then about a month ago, he and I had a call on a Saturday and he's like, \"I think this thing is really real.\" And I called Jonathan and Sunny right away and I said, \"You should be prepared that if this thing works, there's going to be something interesting to be done here.\" And one thing led to another and they acted decisively. Here's what I'll tell you about Jensen. to seeing a little bit up close. He is operating at a level of insight about what's happening in this industry that I've really never seen with other folks in other industries other than Elon and his industries. It's a level of mastery and a level of skill that is really impressive. Obviously, it's a really big deal and I'm really thankful to have been a part of it. Yeah, it's great. >> Slack 25 billion, Grock 20 billion. Two incredible >> but Slack we sold to Salesforce. The thing with rock is I just think that it wasn't a point product. I think what will happen here is if successful we will be a foundational layer that Nvidia can use to again make AI much more accessible, much cheaper, much more beneficial to everybody. >> Jamath invested in Grock to build for a problem that barely existed in 2015. That problem is now the biggest bottleneck in AI. Jim explained that Grock made deliberate design choices from day one. We said, you know, there's an entire technology stack that Nvidia and Google have pioneered. If we try to build a building that looks like that, we will fail. Most chip startups try to beat Nvidia at their own game. More compute, faster processing, better specs on the benchmarks that Nvidia already dominates. Grock did the opposite. They looked at where the bottleneck would be 5 to 10 years out and built specifically for that. Specifically highlighted the decode phase, which is where models generate responses. one token at a time and that is memory constrained, not compute constrained. Nvidia's GPUs aren't optimized for it. Grock's architecture is. Now, I said Jonathan Ross, the CEO of Grog, was a bright spark. Here's some of his hottest AI takes from a recent pod. These are absolute mustatches. Enjoy. I would wager that if OpenAI were given twice the inference compute that they have today, if Anthropic was given twice the inference compute that they have today, that within one month from now their revenue would almost double. I'm sorry, can you unpack that for me? They are compute limited and it and it comes how would their revenue double if they had double the compute? >> Right now, one of the biggest complaints of Enthropic is the rate limits. people can't get enough uh tokens from them. And if they had more compute, they could produce more tokens and they could charge more money. And with OpenAI, it's a chat service. So, how do you regulate your chat service? You run it slower. You get less engagement. I can literally just add more compute to the economy and the economy gets stronger. We've never had that before where it wasn't a bottleneck. was more of a rubberneck where you could just force more of one component through and then everything improves. You said the economy gets stronger. When we think about kind of what that's predicated on, that's predicated on the $10 trillion labor spend in GDP uh shifting um to AI and us taking a portion of that. Do you think that we will see significant shifts in the GDP or the spend on labor moving towards AI in the next 5 years? I believe that AI is going to cause massive labor shortages. Yeah. I I don't think we're going to have enough people to fill all the jobs that are going to be created. There's there's three things that are going to happen because of AI. The first is massive deflationary pressure. Um this cup of coffee is going to cost less. Your housing is going to cost less. Everything is going to cost less, which means people are going to need less money. >> So, how is it going to cost less to have a cup of coffee because of AI? because you're going to have uh robots that are going to be farming the coffee more efficiently. You're going to have better supply chain management. You're going to um it's just going to be across the entire supply chain. Um you're going to be able to genetically engineer the coffee so that you get more of it per um watt of sunlight, right? Just across the entire spectrum. So, you're going to have massive deflationary pressure. That's number one. And what that means is people will need to work less. And that's going to lead you to number two, which is people are going to opt out of the economy more. They're going to work fewer hours. They're going to work fewer days a week, and they're going to work fewer years. They're going to retire earlier because they're going to be able to support their lifestyle working less. And then number three is we're going to create new jobs and new uh company uh new industries that don't exist today. Um think about 100 years ago. 98% of the workforce in the United States was in agriculture. 2% did other things. When we were able to reduce that to 2% of the population working in agriculture, we found things for those other 98% of the population to do. the jobs that are going to exist 100 years from now, we can't even contemplate. 100 years ago, the idea of a software developer made no sense. 100 years from now, it's going to make no sense, but in a different way because everyone's going to be vibe coding, right? Um, and influencers, that wouldn't have made sense 100 years ago. Uh, but now that's a real job. People make millions of dollars off of it. There is no limit to the amount of compute that we can use. It's different from the industrial revolution. In the industrial revolution, um, you couldn't use energy unless you had the machinery to use it and you had to build machinery and that took time. If I wanted to, um, if I wanted to, you know, have more cars on the road, I had to build the cars. It wasn't enough to just pull more oil out of the ground. AI is not like that. Yes, if I make my model better, it I I can actually do more with the same amount of compute. But if I double my compute, I double the number of users. I improve the quality of the model. This is different. I can literally just add more compute to the economy and the economy gets stronger. We've never had that before where it wasn't a bottleneck. It was more of a rubberneck where you could just force more of one component through and then everything improves. I'm like, who here is 100% convinced that in 10 years AI won't be able to do your job? No hands went up. I'm like, great. That's how the hyperscalers feel. So, of course, they're going to be spending like drunken sailors because the alternative is that they're completely locked out of their business. So, it's not a purely economical framework that they're using. It's a do we get to maintain our leadership? Now when you look at it the next step there are these um you know scale law sort of outcomes. You want to remain in the top 10 right? We keep talking about the mag 7. If you're not a member of the mag 7 you're [clears throat] not going to be able to get anywhere near the valuation. And so what do you do to stay there? You spend. And it's worth it because the stock value stays up because you're in the top seven or 10. >> At some point the returns have to be delivered though. the spend has to materialize into actual tangible revenue back and if it doesn't whether you're in the mag 7 or not doesn't matter. Correct. That's correct. But um right now AI is returning massive value already. It's very lumpy in the al in the applications but it's returning massive amounts of value. What I never expected was that AI was going to be based on language. And what that's done is it's made it trivial to interact with AI. I thought it was going to be more like Alph Go. I thought it was going to be intelligent in some weird esoteric way. The fact that it's language means anyone can use it. So I expected AI to come sooner and grow slower. It came later and it's growing faster than I ever imagined. It is so easy to interact with AI that anyone can do it. 10% of the world's population is a GBT weekly active user. >> Isn't that astonishing? >> Yes. But um you know what's holding it back? >> Compute. >> So compute is holding it back for the quality of it. But more people would use it. They just wouldn't get as much out of it. But more people would use it if more languages were supported. I going off and saying I'm going to build my own AI chip to compete with Nvidia. It's a little bit like saying you know that Google search is pretty nice. let's go replicate it. It's insane. Like the level of optimization, the level of design and engineering that goes into that, um, you're not going to be able to replicate it with a high probability of success. However, if there's a bunch of players out there trying to do it and you have optionality and one of them succeeds, then you have another chip. We mentioned earlier that you have to spend if you want to stay in Mac 7. >> Mhm. Nvidia investing a hundred billion dollars into OpenAI for OpenAI just to go and buy back Nvidia chips. Is this not just an infinite money loop? >> Um that would be the case if they weren't spending it with suppliers to build those chips. It's not roundtpping if actual productive outcomes are occurring. So um think of it this way. How what percentage of the spend is going to building that infrastructure? 40%. So at least 40% of those dollars are actually going out into the ecosystem. So that is not an infinite loop. Most people pour money into ads people ignore. YouTube changes that. It builds trust, authority, and a real connection at scale. One law firm we worked with landed 33 clients in just 5 months worth $330,000 from their YouTube channel. If you run a business, this is one of the most overlooked opportunities right now. Book a call with me below and I can show you how we can make it"
    }
]